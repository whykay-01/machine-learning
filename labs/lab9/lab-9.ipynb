{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b64d1cc-b961-48c4-8845-eac0ab19d955",
   "metadata": {},
   "source": [
    "# Fundamentals of Machine Learning (CSCI-UA.473)\n",
    "## Lab 9: Clustering : K-means, Gaussian Mixtures, Density Based Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c7417-94cd-4522-83fe-ea18f8726569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages from sci-kit learn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster import vq # Specifically uesful for K-means clustering\n",
    "from sklearn import cluster  # Clustering algorithms such as K-means and agglomerative\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans \n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "from sklearn.manifold import MDS #Import the multidimensional scaling module\n",
    "from scipy.spatial.distance import squareform #Import squareform, which creates a symmetric matrix from a vector\n",
    "import time\n",
    "import math\n",
    "from sklearn import mixture\n",
    "from scipy.stats import multivariate_normal as normal # Multivariate Gaussian distributions\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import time\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_blobs, fetch_openml, load_iris\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn_som.som import SOM\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca120d85-9095-4720-9c8d-5073bc256457",
   "metadata": {},
   "source": [
    "## K-means Clustering\n",
    "\n",
    "We'll start by looking at the sci-kit learn implementation of K-means for a synthetic dataset that has distinct clusters.  The cell below generates a synthetic dataset with 4 well-separated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd26d07c-5cb3-48f7-98a1-99c10b3eeb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the synthetic dataset.\n",
    "X1 = np.random.multivariate_normal(size = 10, mean = np.array([3, 3]), cov = np.identity(2))\n",
    "X2 = np.random.multivariate_normal(size = 10, mean = np.array([-3, 3]), cov = np.identity(2))\n",
    "X3 = np.random.multivariate_normal(size = 10, mean = np.array([-3, -3]), cov = np.identity(2))\n",
    "X4 = np.random.multivariate_normal(size = 10, mean = np.array([3, -3]), cov = np.identity(2))\n",
    "\n",
    "X = np.vstack([X1,X2,X3,X4])\n",
    "plt.scatter(X1[:,0], X1[:,1], c = 'b', marker = 's', label = 'Cluster 1')\n",
    "plt.scatter(X2[:,0], X2[:,1], c = 'r', marker = 'o', label = 'Cluster 2')\n",
    "plt.scatter(X3[:,0], X3[:,1], c = 'g', marker = 'p', label = 'Cluster 3')\n",
    "plt.scatter(X4[:,0], X4[:,1], c = 'm', marker = '+', label = 'Cluster 4')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title(r'Synthetic dataset with 4 clusters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfe6293-fb00-4e63-91aa-da7a5f696d0c",
   "metadata": {},
   "source": [
    "## Distortion\n",
    "\n",
    "A key quantity in helping determine a good number of clusters to use is the distortion\n",
    "$$\n",
    "J = \\sum_{i=1}^N \\|{\\bf x}_i - \\mu_{C(i)}\\|^2\n",
    "$$\n",
    "where ${\\bf x}_i$ are the data points, $C(i) \\in \\{1,\\ldots,K\\}$ is the cluster assignment for ${\\bf x}_i$ and $\\mu_j$ for $j=1,\\ldots,K$ are the centers of the clusters.  Intuitively, the distortion captures the unexplained variation in the dataset after accounting for the clusters.  If $K = N$, then $\\mu_{C(i)} = {\\bf x}_i$ and the distortion will be 0.  In this case there is a cluster at every data point so intuitively there is no unexplained variation.  However, having a large number of clusters is often not very useful since we will likely be overfitting to noise in the data.  There will often be a certain point where the distortion starts to decrease more slowly.  This is called the \"elbow method\", which is what we plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380c7f5-6cfb-4604-b7dc-5357f7d878de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to keep track of the distortions for K=1,...,N.\n",
    "distortions = np.zeros(len(X))\n",
    "\n",
    "for k in range(1, len(X) + 1):\n",
    "    kmeans = cluster.KMeans(k, n_init=\"auto\") # K-means object in sci-kit learn with k clusters.\n",
    "    kmeans.fit(X)              # This is the line that actually runs the K-means algorithm.\n",
    "    distortions[k-1] = kmeans.inertia_ # In sci-kit learn the distortion is called the inertia.\n",
    "\n",
    "# Plot the results.\n",
    "plt.plot(np.arange(1, len(X)+1, 1), distortions, 'b-x', lw = 2)\n",
    "plt.xlabel(r'Clusters $k$')\n",
    "plt.ylabel(r'Distortion')\n",
    "plt.title(r'Elbow Curve for K-means')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7691f71e-22c4-4d18-9ea6-cd0380513355",
   "metadata": {},
   "source": [
    "Indeed we see from the plot above that the distortion decreases rapidly up until $k=4$, which was the true number of clusters for our data.  After this point we begin overfitting to the noise and the distortion will not decrease as much.  A general heuristic is to choose $k$ where the kink, or elbow, in the curve occurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520890b9-72f0-408b-acb4-a98ee1db52b4",
   "metadata": {},
   "source": [
    "This data was well-separated, however.  Let's see how the distortion behave whenever there is overlap between the clusters.  The cell below generates the new fake dataset with the clusters closer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff6734-137c-4079-9b04-dcb5d1844e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the new synthetic dataset.  The only difference from before is that the means are closer together now.\n",
    "X1 = np.random.multivariate_normal(size = 10, mean = np.array([1, 1]), cov = np.identity(2))\n",
    "X2 = np.random.multivariate_normal(size = 10, mean = np.array([-1, 1]), cov = np.identity(2))\n",
    "X3 = np.random.multivariate_normal(size = 10, mean = np.array([-1, -1]), cov = np.identity(2))\n",
    "X4 = np.random.multivariate_normal(size = 10, mean = np.array([1, -1]), cov = np.identity(2))\n",
    "\n",
    "X = np.vstack([X1,X2,X3,X4])\n",
    "plt.scatter(X1[:,0], X1[:,1], c = 'b', marker = 's', label = 'Cluster 1')\n",
    "plt.scatter(X2[:,0], X2[:,1], c = 'r', marker = 'o', label = 'Cluster 2')\n",
    "plt.scatter(X3[:,0], X3[:,1], c = 'g', marker = 'p', label = 'Cluster 3')\n",
    "plt.scatter(X4[:,0], X4[:,1], c = 'm', marker = '+', label = 'Cluster 4')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title(r'Synthetic dataset with 4 clusters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec2f78d-0bf5-4e03-b782-06e034c3bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of the distortions for k=1,...,N.\n",
    "distortions = np.zeros(len(X))\n",
    "\n",
    "# Same code as before.\n",
    "for k in range(1, len(X) - 1):\n",
    "    kmeans = cluster.KMeans(k, n_init='auto')\n",
    "    kmeans.fit(X)\n",
    "    distortions[k-1] = kmeans.inertia_\n",
    "\n",
    "# Plot the results.\n",
    "plt.plot(np.arange(1, len(X)+1, 1), distortions, 'b-x', lw = 2)\n",
    "plt.xlabel(r'Clusters $k$')\n",
    "plt.ylabel(r'Distortion')\n",
    "plt.title(r'Elbow Curve for K-means')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0201499-e5aa-4a8d-991a-e6a6a2cad7c4",
   "metadata": {},
   "source": [
    "Now there is a more gradual decrease in the distortion and it is not as clear what choice of $k$ one should use.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5695d25a-9dc0-46c4-abcd-4f649b5cf78c",
   "metadata": {},
   "source": [
    "# Silhoutte Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e7a08-307b-4d2d-9e63-6389bebd09b4",
   "metadata": {},
   "source": [
    "![](Sscore.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f91ba3-27ba-4fa8-a16d-aa061a5bf688",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouettes = np.zeros(len(X))\n",
    "\n",
    "# Same code as before.\n",
    "for k in range(2, len(X) - 1):\n",
    "    kmeans = cluster.KMeans(k, n_init='auto')\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    silhouettes[k-1] = silhouette_score(X, labels)\n",
    "    \n",
    "plt.plot(np.arange(1, len(X)+1, 1), silhouettes, 'r-o', lw = 2)\n",
    "plt.xlabel(r'Clusters $k$')\n",
    "plt.ylabel(r'Silhouette Average')\n",
    "plt.title(r'Elbow Curve for K-means')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ee5b7-250c-4713-8d14-07f15b81fd9f",
   "metadata": {},
   "source": [
    "## Initializing the Cluster Centers \n",
    "One feature of K-means is that it is prone to becoming stuck in local minimum and is therefore sensitive to the initial cluster centers that are chosen.  The scipy implementation `kmeans2` allows for more flexibility in choosing the initial conditions so we also show it here as an alternative to sci-kit's implementation.  Now we see how the distortion changes after each iteration in K-means for different initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b72fb-0cf5-49d9-a4f0-e31aebb774cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distortion vs iteration for three different initial means.\n",
    "\n",
    "# Helper function to compute the distortion using the data X and the computed cluster centers\n",
    "# and labels for each point.\n",
    "def distortion(X, centers, labels):\n",
    "    N = X.shape[0]\n",
    "    J = 0\n",
    "    for i in range(N):\n",
    "        J += np.linalg.norm(X[i] - centers[labels[i]])**2\n",
    "    return J\n",
    "\n",
    "# Only use 10 iterations of K-means.\n",
    "max_iter = 10\n",
    "distortions_1 = np.zeros(max_iter)\n",
    "distortions_2 = np.zeros(max_iter)\n",
    "distortions_3 = np.zeros(max_iter)\n",
    "\n",
    "# 3 different initializations.\n",
    "K = 4 # 4 clusters\n",
    "np.random.seed(325) # Random seed is only chosen to emphasize difference between initializations.\n",
    "                    # This line could be removed.\n",
    "centers1, labels1 = vq.kmeans2(data = X, k = K, iter = 1, minit = '++')      # k-means++ initialization\n",
    "centers2, labels2 = vq.kmeans2(data = X, k = K, iter = 1, minit = 'random')  # points sampled from a Gaussian\n",
    "centers3, labels3 = vq.kmeans2(data = X, k = K, iter = 1, minit = 'points')  # points chosen from the dataset\n",
    "\n",
    "distortions_1[0] = distortion(X, centers1, labels1)\n",
    "distortions_2[0] = distortion(X, centers2, labels2)\n",
    "distortions_3[0] = distortion(X, centers3, labels3)\n",
    "\n",
    "for i in range(1, max_iter):\n",
    "    # Do 1 iteration of K-means using the cluster centers from the last iteration.\n",
    "    centers1, labels1 = vq.kmeans2(data = X, k = centers1, iter = 1, minit = 'matrix')\n",
    "    centers2, labels2 = vq.kmeans2(data = X, k = centers2, iter = 1, minit = 'matrix')\n",
    "    centers3, labels3 = vq.kmeans2(data = X, k = centers3, iter = 1, minit = 'matrix')\n",
    "    \n",
    "    distortions_1[i] = distortion(X, centers1, labels1)\n",
    "    distortions_2[i] = distortion(X, centers2, labels2)\n",
    "    distortions_3[i] = distortion(X, centers3, labels3)\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(np.arange(1, max_iter + 1), distortions_1, 'r-s', label = '++')\n",
    "plt.plot(np.arange(1, max_iter + 1), distortions_2, 'b-o', label = 'random')\n",
    "plt.plot(np.arange(1, max_iter + 1), distortions_3, 'm-x', label = 'points')\n",
    "plt.xlabel(r'Iteration')\n",
    "plt.ylabel(r'Distortion')\n",
    "plt.title(r'Convergence of K-means for different initializations')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc0c83-0236-4c65-8e41-e2d02d4c3945",
   "metadata": {},
   "source": [
    "The fact that all three curves have leveled-off indicates the K-means has converged.  However, the distortion is different meaning we have congevered to different local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aacfbe8-deac-47ba-9803-4af9b9cec55b",
   "metadata": {},
   "source": [
    "# K-Mediods Clustering <a class=\"anchor\" id=\"second\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21a5bf-371b-4c71-b7a7-2ff746125703",
   "metadata": {},
   "source": [
    "The k-medoids algorithm is a clustering algorithm related to the k-means algorithm and the medoidshift algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups). K-means attempts to minimize the total squared error, while k-medoids minimizes the sum of dissimilarities between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses datapoints as centers ( medoids or exemplars).\n",
    "\n",
    "K-medoids is also a partitioning technique of clustering that clusters the data set of n objects into k clusters with k known a priori. A useful tool for determining k is the silhouette.\n",
    "\n",
    "It could be more robust to noise and outliers as compared to k-means because it minimizes a sum of general pairwise dissimilarities instead of a sum of squared Euclidean distances. The possible choice of the dissimilarity function is very rich but in our applet we used the Euclidean distance.\n",
    "\n",
    "A medoid of a finite dataset is a data point from this set, whose average dissimilarity to all the data points is minimal i.e. it is the most centrally located point in the set.\n",
    "\n",
    "The most common realisation of k-medoid clustering is the Partitioning Around Medoids (PAM) algorithm and is as follows:\n",
    "\n",
    "    * Initialize: randomly select k of the n data points as the medoids\n",
    "    * Assignment step: Associate each data point to the closest medoid.\n",
    "    * Update step: For each medoid m and each data point o associated to m swap m and o and compute the total cost of the configuration (that is, the average dissimilarity of o to all the data points associated to m). Select the mediod o with the lowest cost of the configuration.\n",
    "\n",
    "Repeat alternating steps 2 and 3 until there is no change in the assignments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e1d984-34b4-4aeb-9064-8217d0d58207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define distance metric\n",
    "euclidean = lambda x1,x2: np.sqrt(np.sum((x1-x2)**2,-1))\n",
    "manhattan = lambda x1,x2: np.sum(np.abs(x1-x2), -1)\n",
    "hamming = lambda x1,x2: np.sum((x1!=x2),-1)\n",
    "dist_dict={\"euclidean\":euclidean, \"manhattan\":manhattan, \"hamming\":hamming}\n",
    "class KMedoids:\n",
    "    \n",
    "    def __init__(self, K=5, max_iters=100, dist_fn=euclidean, repeats=10):\n",
    "        self.K = K\n",
    "        self.max_iters = max_iters                        #for computing each medoid\n",
    "        self.dist_fn = dist_fn\n",
    "        self.repeats = repeats                            #for several runs to compute medoids\n",
    "    \n",
    "    def fit(self, x):\n",
    "        #note that medoids stores a list of K indices \n",
    "        n,d = x.shape\n",
    "        distances = self.dist_fn(x[None,:,:], x[:,None,:])       #distance function for pairwise distance [n, n]\n",
    "        best_cost = np.inf\n",
    "        for r in range(self.repeats):\n",
    "        #we repeat the process of finding medoids for self.repeats iterations\n",
    "            medoids = np.random.choice(n, self.K, replace=False)      #randomly choose a list of k distinct indices from 0 to n-1\n",
    "            for t in range(self.max_iters):\n",
    "                membership = np.argmin(distances[medoids,:], axis=0)       #assign membership based on distance from the medoids\n",
    "                new_medoids = medoids.copy()\n",
    "                cost = 0\n",
    "                for i in range(self.K):\n",
    "                    cluster_inds = np.nonzero(membership == i)[0]                                    #returns the indices of points with membership i\n",
    "                    cluster_dist = np.sum(distances[np.ix_(cluster_inds, cluster_inds)], axis=1)     #pairwise distance between points with membership i and summed over axis 1  \n",
    "                    cost += np.min(cluster_dist)                                                     #compute the cost  for the \n",
    "                    new_medoids[i] = cluster_inds[np.argmin(cluster_dist)]                           #find the index of i-th medoids\n",
    "                if np.allclose(new_medoids, medoids):\n",
    "                    #print(f'converged after {t} iterations with the cost {cost}')\n",
    "                    break\n",
    "                medoids = new_medoids\n",
    "            if cost < best_cost:\n",
    "                best_medoids = medoids\n",
    "                best_membership = membership\n",
    "        return best_medoids, best_membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c889364e-19ee-4e72-917b-c9060f075906",
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-Mediods Visualization\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = datasets.make_blobs(\n",
    "    n_samples=750, centers=centers, cluster_std=0.4, random_state=0\n",
    ")\n",
    "kmediods = KMedoids(K=3)\n",
    "mediods, labels = kmediods.fit(X)\n",
    "unique_labels = set(labels)\n",
    "fig = plt.figure(figsize =(12, 10))\n",
    "colors = [\n",
    "    plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))\n",
    "]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = X[class_member_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=6,\n",
    "    )\n",
    "plt.plot(\n",
    "    X[mediods][:, 0],\n",
    "    X[mediods][:, 1],\n",
    "    \"o\",\n",
    "    markerfacecolor=\"cyan\",\n",
    "    markeredgecolor=\"k\",\n",
    "    markersize=6,\n",
    ")\n",
    "\n",
    "plt.title(\"KMedoids clustering. Medoids are represented in cyan.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209fcce0-0996-4447-8363-de58851f3ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking the representative digits that we get based on the dissimilarity measure used.\n",
    "x_org, y = datasets.fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "def plot_digits(data):\n",
    "    num_plots = data.shape[0]\n",
    "    fig = plt.figure(figsize=(num_plots, 10.*num_plots))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(1, num_plots), axes_pad=0.1)\n",
    "    for i in range(num_plots):\n",
    "        grid[i].imshow(data[i].reshape((28,28)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c43b2ba-1233-4a92-96b3-3e71b88895a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_org.values[:1000]\n",
    "for distance in dist_dict.keys():\n",
    "    kmedoid = KMedoids(10, dist_fn=dist_dict[distance], repeats=1000)\n",
    "    centers, _ = kmedoid.fit(x.reshape(-1, 784))\n",
    "    print(\"Distance Function used: \", distance)\n",
    "    plot_digits(x[centers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c22d7-79a8-4d22-a426-601e40acc5d2",
   "metadata": {},
   "source": [
    "## Why to use KMediods over KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aa89e8-0588-4d63-94e8-5c2a2fdd8127",
   "metadata": {},
   "source": [
    "Pros:\n",
    "1. _K-medoid is more flexible_: First of all, you can use k-medoids with any similarity measure. K-means however, may fail to converge - it really must only be used with distances that are consistent with the mean. So e.g. Absolute Pearson Correlation must not be used with k-means, but it works well with k-medoids.\n",
    "\n",
    "2. _Robustness of medoid_: Secondly, the medoid as used by k-medoids is roughly comparable to the median (in fact, there also is k-medians, which is like K-means but for Manhattan distance). If you look up literature on the median, you will see plenty of explanations and examples why the median is more robust to outliers than the arithmetic mean. Essentially, these explanations and examples will also hold for the medoid. It is a more robust estimate of a representative point than the mean as used in k-means.\n",
    "\n",
    "Cons:\n",
    "1. _k-medoids is much more expensive_: That's the main drawback. Usually, PAM takes much longer to run than k-means. As it involves computing all pairwise distances, it is O(n^2*k*i); whereas k-means runs in O(n*k*i) where usually, k times the number of iterations is k*i << n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1a652-942f-48d6-9354-7660a2237c9b",
   "metadata": {},
   "source": [
    "## Gaussian mixture models and the EM algorithm\n",
    "\n",
    "One disadvantage of K-means is that the clusters are restricted to be spherical and so it is heavily dependent on the scaling of the features.  A Gaussian Mixture Model (GMMs) has an extra covariance parameter which allows us to represent poorly-scaled (i.e. long ellipses) data. In addition, GMMs also allow for soft clustering where any point could be contributing to multiple clusters. \n",
    "\n",
    "Consider the following toy dataset generated from the Gaussian mixture model with parameters\n",
    "$$\n",
    "\\phi = (0.5,\\ 0.5),\\quad \\mu_0 = \\begin{bmatrix}-3\\\\0\\end{bmatrix},\\quad \\mu_1 = \\begin{bmatrix}3\\\\0\\end{bmatrix}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\Sigma_0 = \\Sigma_1 = \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 100\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The dataset is plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12b22b-31e4-4a5c-a984-a89d4983e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "mu1 = np.array([-3, 0])\n",
    "mu2 = np.array([3, 0])\n",
    "Sigma = np.array([[1, 0], [0, 100]])\n",
    "X1 = normal.rvs(mean = mu1, cov = Sigma, size = 25)\n",
    "X2 = normal.rvs(mean = mu2, cov = Sigma, size = 25)\n",
    "\n",
    "# Merge the data together.\n",
    "X = np.vstack([X1, X2])\n",
    "\n",
    "# Plot the data.\n",
    "plt.scatter(X1[:,0], X1[:,1], c = 'b', marker = 'o', label = 'Cluster 1')\n",
    "plt.scatter(X2[:,0], X2[:,1], c = 'r', marker = '+', label = 'Cluster 2')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title(r'Toy Dataset from GMM')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4e8e4-d9e3-42d2-bd95-9458bc5bf5c8",
   "metadata": {},
   "source": [
    "Now we'll fit a Gaussian mixture model to this data and plot the contours for the covariance as well as the locations of the means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397ee86-8fa5-4180-a852-18c695fa7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Gaussian mixture model object with 2 components.  The 'tied' argument refers to the fact that we\n",
    "# specify the covariances of both components to be the same.  This constrains the problem and reduces the \n",
    "# number of parameters we need to estimate.  We also provide the initial parameters which we happen to know already.\n",
    "gmm = mixture.GaussianMixture(n_components = 2, covariance_type = 'tied', tol=1e-3, \\\n",
    "                              means_init = [mu1, mu2], precisions_init = np.linalg.inv(Sigma))\n",
    "\n",
    "# The fit function uses the EM algorithm.\n",
    "gmm.fit(X)\n",
    "\n",
    "# Get the fitted means and covariances.\n",
    "phi = gmm.weights_\n",
    "mu = gmm.means_\n",
    "cov = gmm.covariances_\n",
    "\n",
    "# Can also get the number of iterations that was needed for convergence.\n",
    "print(\"{:d} iterations for EM to converge.\".format(gmm.n_iter_))\n",
    "\n",
    "# Make a contour plot of the data.\n",
    "xx = np.linspace(-6, 6, 100)\n",
    "yy = np.linspace(-20, 20, 100)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "points = np.asarray([np.ravel(XX), np.ravel(YY)]).T\n",
    "Z = phi[0]*normal.pdf(points, mean = mu[0], cov = cov) + phi[1]*normal.pdf(points, mean = mu[1], cov = cov)\n",
    "ZZ = Z.reshape(XX.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.contour(XX, YY, ZZ)\n",
    "plt.scatter(X1[:,0], X1[:,1], c = 'b', marker = 'o', label = 'Cluster 1')\n",
    "plt.scatter(X2[:,0], X2[:,1], c = 'r', marker = '+', label = 'Cluster 2')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title(r'Toy Dataset from GMM fitted with EM algorithm')\n",
    "plt.legend()\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "print(mu1, mu2, mu, phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1889d7-e7b4-4a24-be49-73f656ff36f3",
   "metadata": {},
   "source": [
    "# Density-based spatial clustering of applications with noise (DBSCAN) <a class=\"anchor\" id=\"third\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef55231-e20f-40f3-bfdb-300b5b20affa",
   "metadata": {},
   "source": [
    "DBSCAN requires two parameters: ε (eps) and the minimum number of points required to form a dense region (minPts). It starts with an arbitrary starting point that has not been visited. This point's ε-neighborhood is retrieved, and if it contains sufficiently many points, a cluster is started. Otherwise, the point is labeled as noise. Note that this point might later be found in a sufficiently sized ε-environment of a different point and hence be made part of a cluster.\n",
    "\n",
    "If a point is found to be a dense part of a cluster, its ε-neighborhood is also part of that cluster. Hence, all points that are found within the ε-neighborhood are added, as is their own ε-neighborhood when they are also dense. This process continues until the density-connected cluster is completely found. Then, a new unvisited point is retrieved and processed, leading to the discovery of a future cluster or noise.\n",
    "\n",
    "DBSCAN can be used with any distance function (as well as similarity functions or other predicates). The distance function (dist) can therefore be seen as an additional parameter.\n",
    "\n",
    "The algorithm can be expressed in pseudocode as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9104a00-ad7d-4c7c-ae78-fd6b61776733",
   "metadata": {},
   "source": [
    "```\n",
    "DBSCAN(DB, distFunc, eps, minPts) {\n",
    "    C := 0                                                  /* Cluster counter */\n",
    "    for each point P in database DB {\n",
    "        if label(P) ≠ undefined then continue               /* Previously processed in inner loop */\n",
    "        Neighbors N := RangeQuery(DB, distFunc, P, eps)     /* Find neighbors */\n",
    "        if |N| < minPts then {                              /* Density check */\n",
    "            label(P) := Noise                               /* Label as Noise */\n",
    "            continue\n",
    "        }\n",
    "        C := C + 1                                          /* next cluster label */\n",
    "        label(P) := C                                       /* Label initial point */\n",
    "        SeedSet S := N \\ {P}                                /* Neighbors to expand */\n",
    "        for each point Q in S {                             /* Process every seed point Q */\n",
    "            if label(Q) = Noise then label(Q) := C          /* Change Noise to border point */\n",
    "            if label(Q) ≠ undefined then continue           /* Previously processed (e.g., border point) */\n",
    "            label(Q) := C                                   /* Label neighbor */\n",
    "            Neighbors N := RangeQuery(DB, distFunc, Q, eps) /* Find neighbors */\n",
    "            if |N| ≥ minPts then {                          /* Density check (if Q is a core point) */\n",
    "                S := S ∪ N                                  /* Add new neighbors to seed set */\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "where RangeQuery is:\n",
    "\n",
    "```\n",
    "RangeQuery(DB, distFunc, Q, eps) {\n",
    "    Neighbors N := empty list\n",
    "    for each point P in database DB {                      /* Scan all points in the database */\n",
    "        if distFunc(Q, P) ≤ eps then {                     /* Compute distance and check epsilon */\n",
    "            N := N ∪ {P}                                   /* Add to result */\n",
    "        }\n",
    "    }\n",
    "    return N\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af155c-d5c8-4746-8e80-e4e792a01a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "crater = pd.read_csv('crater.csv')\n",
    "assert len (crater) == 1500\n",
    "assert set (crater.columns) == set (['x_1', 'x_2', 'kmeans_label'])\n",
    "\n",
    "with open ('crater_counts.txt', 'rt') as fp:\n",
    "    true_counts = [int (c) for c in fp.read ().split (',')]\n",
    "    assert sum (crater['kmeans_label'] == 0) == true_counts[0]\n",
    "    assert sum (crater['kmeans_label'] == 1) == true_counts[1]\n",
    "\n",
    "def make_scatter_plot (df, x=\"x_1\", y=\"x_2\", hue=\"label\",\n",
    "                       palette={0: \"red\", 1: \"olive\", 2: \"blue\", 3: \"green\"},\n",
    "                       size=5,\n",
    "                       centers=None):\n",
    "    if (hue is not None) and (hue in df.columns):\n",
    "        sns.lmplot (x=x, y=y, hue=hue, data=df, palette=palette,\n",
    "                    fit_reg=False)\n",
    "    else:\n",
    "        sns.lmplot (x=x, y=y, data=df, fit_reg=False)\n",
    "\n",
    "    if centers is not None:\n",
    "        plt.scatter (centers[:,0], centers[:,1],\n",
    "                     marker=u'*', s=500,\n",
    "                     c=[palette[0]])\n",
    "\n",
    "def make_scatter_plot2 (df, x=\"x_1\", y=\"x_2\", hue=\"label\", size=5):\n",
    "    if (hue is not None) and (hue in df.columns):\n",
    "        sns.lmplot (x=x, y=y, hue=hue, data=df,\n",
    "                    fit_reg=False)\n",
    "    else:\n",
    "        sns.lmplot (x=x, y=y, data=df, fit_reg=False)\n",
    "\n",
    "make_scatter_plot (crater, hue='kmeans_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e652fe-4c2d-4328-8261-8a71b3cb1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DBSCAN Algorithm\n",
    "def region_query (p, eps, X):\n",
    "    # These lines check that the inputs `p` and `X` have\n",
    "    # the right shape.\n",
    "    _, dim = X.shape\n",
    "    assert (p.shape == (dim,)) or (p.shape == (1, dim)) or (p.shape == (dim, 1))\n",
    "    \n",
    "    return np.linalg.norm (p - X, axis=1) <= eps\n",
    "\n",
    "def index_set (y):\n",
    "    \"\"\"\n",
    "    Given a boolean vector, this function returns\n",
    "    the indices of all True elements.\n",
    "    \"\"\"\n",
    "    assert len (y.shape) == 1\n",
    "\n",
    "    return set (np.where (y)[0])\n",
    "\n",
    "def find_neighbors (eps, X):\n",
    "    m, d = X.shape\n",
    "    neighbors = [] # Empty list to start\n",
    "    for i in range (len (X)):\n",
    "        n_i = index_set (region_query (X[i, :], eps, X))\n",
    "        neighbors.append (n_i)\n",
    "    assert len (neighbors) == m\n",
    "    return neighbors\n",
    "\n",
    "def find_core_points (s, neighbors):\n",
    "    assert type (neighbors) is list\n",
    "    assert all ([type (n) is set for n in neighbors])\n",
    "    \n",
    "    core_set = set ()\n",
    "    for i, n_i in enumerate (neighbors):\n",
    "        if len (n_i) >= s:\n",
    "            core_set.add (i)\n",
    "    return core_set\n",
    "\n",
    "def expand_cluster (p, neighbors, core_set, visited, assignment):\n",
    "    # Assume the caller performs Steps 1 and 2 of the procedure.\n",
    "    # That means 'p' must be a core point that is part of a cluster.\n",
    "    assert (p in core_set) and (p in visited) and (p in assignment)\n",
    "    \n",
    "    reachable = set (neighbors[p])  # Step 3\n",
    "    while reachable:\n",
    "        q = reachable.pop () # Step 4\n",
    "\n",
    "        if q not in visited:\n",
    "            visited.add (q) # Mark q as visited\n",
    "            if q in core_set:\n",
    "                reachable |= neighbors[q]\n",
    "        if q not in assignment:\n",
    "            assignment[q] = assignment[p]\n",
    "        \n",
    "    # This procedure does not return anything\n",
    "    # except via updates to `visited` and\n",
    "    # `assignment`.\n",
    "    \n",
    "def dbscan (eps, s, X):\n",
    "    clusters = []\n",
    "    point_to_cluster = {}\n",
    "    \n",
    "    neighbors = find_neighbors (eps, X)\n",
    "    core_set = find_core_points (s, neighbors)\n",
    "    \n",
    "    assignment = {}\n",
    "    next_cluster_id = 0\n",
    "\n",
    "    visited = set ()\n",
    "    for i in core_set: # for each core point i\n",
    "        if i not in visited:\n",
    "            visited.add (i) # Mark i as visited\n",
    "            assignment[i] = next_cluster_id\n",
    "            expand_cluster (i, neighbors, core_set,\n",
    "                            visited, assignment)\n",
    "            next_cluster_id += 1\n",
    "\n",
    "    return assignment, core_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e096bc-b82a-4be3-9f64-50a927f843e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization\n",
    "X = crater[['x_1', 'x_2']].values\n",
    "assignment, core_set = dbscan (0.73, 50, X)\n",
    "\n",
    "print (\"Number of core points:\", len (core_set))\n",
    "print (\"Number of clusters:\", max (assignment.values ()))\n",
    "print (\"Number of unclassified points:\", len (X) - len (assignment))\n",
    "\n",
    "def plot_labels (df, labels):\n",
    "    df_labeled = df.copy ()\n",
    "    df_labeled['label'] = labels\n",
    "    make_scatter_plot2 (df_labeled)\n",
    "\n",
    "labels = [-1] * len (X)\n",
    "for i, c in assignment.items ():\n",
    "    labels[i] = c\n",
    "plot_labels (crater, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c11a7a-d099-4aca-8efa-60cf5176641f",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering <a class=\"anchor\" id=\"fifth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02d440-e66d-43fa-8434-46c3b4f9c9b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hierarchical clustering involves creating clusters that have a predetermined ordering from top to bottom. For example, all files and folders on the hard disk are organized in a hierarchy. There are two types of hierarchical clustering, Divisive and Agglomerative.\n",
    "\n",
    "**Divisive method**\n",
    "\n",
    "In this method we assign all of the observations to a single cluster and then partition the cluster to two least similar clusters. Finally, we proceed recursively on each cluster until there is one cluster for each observation.\n",
    "\n",
    "**Agglomerative method**\n",
    "\t\t\n",
    "In this method we assign each observation to its own cluster. Then, compute the similarity (e.g., distance) between each of the clusters and join the two most similar clusters. Finally, repeat steps 2 and 3 until there is only a single cluster left.\n",
    "\n",
    "## Linkage or distance matrix\n",
    "\n",
    "Before any clustering is performed, it is required to determine the proximity matrix containing the distance between each point using a distance function. Then, the matrix is updated to display the distance between each cluster. The following three methods differ in how the distance between each cluster is measured.\n",
    "\n",
    "**Single Linkage** \t\t\n",
    "In single linkage hierarchical clustering, the distance between two clusters is defined as the shortest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two closest points.\n",
    "<img src=http://www.saedsayad.com/images/Clustering_single.png>\n",
    "\n",
    "**Complete Linkage**\t\t\n",
    "In complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two furthest points.\n",
    "<img src=http://www.saedsayad.com/images/Clustering_complete.png>\n",
    "\n",
    "**Average Linkage**\t\n",
    "In average linkage hierarchical clustering, the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster. For example, the distance between clusters “r” and “s” to the left is equal to the average length each arrow between connecting the points of one cluster to the other.\n",
    "<img src=http://www.saedsayad.com/images/Clustering_average.png>\n",
    "\n",
    "## Dendograms\n",
    "\n",
    "[Dendograms](https://en.wikipedia.org/wiki/Dendrogram) are tree diagrams frequently used to illustrate the arrangement of the clusters produced by hierarchical clustering. The clades are arranged according to how similar (or dissimilar) they are. Clades that are close to the same height are similar to each other; clades with different heights are dissimilar — the greater the difference in height, the more dissimilarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008e428-bc86-4fe9-9466-3b8135534836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Mall_Customers.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a4a287-4e64-4fdd-9d64-658ab239eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f3867-60e0-4608-bd67-a29bae080450",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Annual income distribution\",fontsize=16)\n",
    "plt.xlabel (\"Annual income (k$)\",fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.hist(df['Annual Income (k$)'],color='orange',edgecolor='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361319a6-9a4e-49d3-9014-f9019f6abffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Spending Score distribution\",fontsize=16)\n",
    "plt.xlabel (\"Spending Score (1-100)\",fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.hist(df['Spending Score (1-100)'],color='green',edgecolor='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc0506-4471-4a5b-98e1-bc43619dea5b",
   "metadata": {},
   "source": [
    "### So, is there a definitive correlation between annual income and spending score? - *Apparently not*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4313509-22bf-4cc5-96a8-71299c560494",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Annual Income and Spending Score correlation\",fontsize=18)\n",
    "plt.xlabel (\"Annual Income (k$)\",fontsize=14)\n",
    "plt.ylabel (\"Spending Score (1-100)\",fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.scatter(df['Annual Income (k$)'],df['Spending Score (1-100)'],color='red',edgecolor='k',alpha=0.6, s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fc6e43-a76a-41fc-a956-4b8cf52d4528",
   "metadata": {},
   "source": [
    "### How about correlation between age and spending score? - *Apparently not*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41751670-07ae-43d3-a98f-69a04cc1a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Age and Spending Score correlation\",fontsize=18)\n",
    "plt.xlabel (\"Age\",fontsize=14)\n",
    "plt.ylabel (\"Spending Score (1-100)\",fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.scatter(df['Age'],df['Spending Score (1-100)'],color='blue',edgecolor='k',alpha=0.6, s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca8872-9088-4c60-8460-ef04fc94a23d",
   "metadata": {},
   "source": [
    "## Strategy\n",
    "** Therefore, we will explore cluserting the customers based on their annual income and spending score to see if there are distinguisbale clusters which the mall can target **\n",
    "\n",
    "We could use k-means but we don't have any idea about the number of hidden clusters. We will see that hierarchial clustering with dendograms will give us a good insight on the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2f8cf1-fcf6-4489-b47c-3139227b640b",
   "metadata": {},
   "source": [
    "## Dendograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb8418-7b69-4ade-ad88-f13db4d1070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,[3,4]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af76242-da40-4c40-9d5d-86594c1e14de",
   "metadata": {},
   "source": [
    "### _Ward_ distance matrix\n",
    "We will use 'Ward' distance matrix for this dendogram.\n",
    "$$d(u,v) = \\sqrt{\\frac{|v|+|s|}{T}d(v,s)^2+ \\frac{|v|+|t|}{T}d(v,t)^2- \\frac{|v|}{T}d(s,t)^2}$$\n",
    "\n",
    "where **$u$** is the newly joined cluster consisting of clusters **$s$** and **$t$**, **$v$** is an unused cluster in the forest, **$T=|v|+|s|+|t|$**, and **$|*|$** is the cardinality of its argument. This is also known as the incremental algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080341c6-a726-4dab-b7d6-000b0404c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "#plt.grid(True)\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440b5ef-b556-49ea-95b8-54a44cf06f68",
   "metadata": {},
   "source": [
    "### Optimal number of clusters\n",
    "\n",
    "Often, the optimal number of clusters can be found from a Dendogram is a simple manner.\n",
    "* Look for the longest stretch of vertical line which is not crossed by any ***extended*** horizontal lines (here *extended* means horizontal lines i.e. the cluster dividers are extended infinitely to both directions).\n",
    "* Now take any point on that stretch of line and draw an imaginary horizontal line.\n",
    "* Count how many vertical lines this imaginary lines crossed.\n",
    "* That is likely to be the optimal number of clusters.\n",
    "\n",
    "**The idea is shown in the following figure. Here the optimal number of clusters could be 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a33d5-ddf5-426a-b7d1-309356882242",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.hlines(y=190,xmin=0,xmax=2000,lw=3,linestyles='--')\n",
    "plt.text(x=900,y=220,s='Horizontal line crossing 5 vertical lines',fontsize=20)\n",
    "#plt.grid(True)\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1873a7a8-62aa-461f-90ad-7cb145eb9695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc = AgglomerativeClustering(n_clusters = 5, metric = 'euclidean', linkage = 'ward')\n",
    "y_hc = hc.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4102652-b56f-4bf1-8227-2785980647c7",
   "metadata": {},
   "source": [
    "### Plot the clusters and label customer types\n",
    "* _Careful_ - high income but low spenders\n",
    "* _Standard_ - middle income and middle spenders\n",
    "* **_Target group_ - middle-to-high income and high spenders (should be targeted by the mall)**\n",
    "* _Careless_ - low income but high spenders (should be avoided because of possible credit risk)\n",
    "* _Sensible_ - low income and low spenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b57359-c340-4b54-98fd-fe571d57cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Careful')\n",
    "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Standard')\n",
    "plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Target group')\n",
    "plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'orange', label = 'Careless')\n",
    "plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Sensible')\n",
    "plt.title('Clustering of customers',fontsize=20)\n",
    "plt.xlabel('Annual Income (k$)',fontsize=16)\n",
    "plt.ylabel('Spending Score (1-100)',fontsize=16)\n",
    "plt.legend(fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.axhspan(ymin=60,ymax=100,xmin=0.4,xmax=0.96,alpha=0.3,color='yellow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd4a47d-82d4-498d-99ab-03781bfe5780",
   "metadata": {},
   "source": [
    "## Verifying the optimal number of clusters by k-means algorithm\n",
    "\n",
    "Given a set of observations $(x_1, x_2, …, x_n)$, where each observation is a d-dimensional real vector, [**k-means clustering**](https://en.wikipedia.org/wiki/K-means_clustering) aims to partition the *$n$* observations into *$k$* (≤ *$n$*) sets $S = {S_1, S_2, …, S_k}$ so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:\n",
    "\n",
    "$${\\displaystyle {\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}\\sum _{\\mathbf {x} \\in S_{i}}\\left\\|\\mathbf {x} -{\\boldsymbol {\\mu }}_{i}\\right\\|^{2}={\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}|S_{i}|\\operatorname {Var} S_{i}}$$\n",
    "\n",
    "where $\\mu_i$ is the mean of points in $S_i$\n",
    "\n",
    "We run k-means++ model (k-means with carefully initialized centroids) iterating over number of clusters (1 to 15) and plot the ***within-cluster-sum-of-squares (WCSS) matric*** to determine the optimum number of cluster by elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8893790-6f00-42a9-bef8-aee15b527862",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 16):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++')\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "with plt.style.context(('fivethirtyeight')):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range(1, 16), wcss)\n",
    "    plt.title('The Elbow Method with k-means++\\n',fontsize=25)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.ylabel('WCSS (within-cluster sums of squares)')\n",
    "    plt.vlines(x=5,ymin=0,ymax=250000,linestyles='--')\n",
    "    plt.text(x=5.5,y=110000,s='5 clusters seem optimal choice \\nfrom the elbow position',\n",
    "             fontsize=25,fontdict={'family':'Times New Roman'})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426a90d-60eb-49c9-b535-5fae85050f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
